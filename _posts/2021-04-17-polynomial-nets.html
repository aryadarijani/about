<!--/ The source code is inspired by (and adapted from) https://github.com/nerfies/nerfies.github.io/blob/main/index.html . -->
<section class="hero">
<div class="hero-body">
<div class="container">
<div class="columns is-centered">
<div class="column has-text-centered">
<h1 class="title is-1 publication-title">Deep Polynomial Neural Networks</h1>
<div class="is-size-5 publication-authors"><span class="author-block"> <a href="https://grigorisg9gr.github.io/">Grigorios Chrysos</a><sup>1</sup>,</span> <span class="author-block"> <a>Stylianos Moschoglou</a><sup>2</sup>,</span> <span class="author-block"> <a>Giorgos Bouritsas</a><sup>2</sup>, </span> <span class="author-block"> <a>Jiankang Deng</a><sup>2</sup>, </span> <span class="author-block"> <a href="https://www.doc.ic.ac.uk/~ipanagak/">Yannis Panagakis</a><sup>3</sup>, </span> <span class="author-block"> <a href="https://wp.doc.ic.ac.uk/szafeiri/">Stefanos Zafeiriou</a><sup>2</sup> </span></div>
<div class="is-size-5 publication-authors"><span class="author-block"><sup>1</sup>&Eacute;cole Polytechnique F&eacute;d&eacute;rale de Lausanne (EPFL),</span> <span class="author-block"><sup>2</sup>Imperial College London,</span> <span class="author-block"><sup>3</sup>University of Athens</span></div>
<div class="column has-text-centered">
<div class="publication-links"><!-- PDF Link. --> <a style="color: white; text-decoration: none;" href="https://ieeexplore.ieee.org/document/9353253"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;Paper</span></a> 
      <a style="color: white; text-decoration: none;" href="https://arxiv.org/abs/2006.13026"><span class="buttong button-round">Paper (open access)</span></a> 
      <a style="color: white; text-decoration: none;" href="https://github.com/grigorisg9gr/polynomial_nets"><span class="buttong button-round"> <i class="fa fa-fw fa-github" aria-hidden="true"></i>&nbsp;Code</span></a>
      <a style="color: white; text-decoration: none;" href="/files/publications/pi-net_cvpr_poster.pdf"><span class="buttong button-round"> <i class="fa fa-fw fa-file-text-o" aria-hidden="true"></i>&nbsp;Poster</span></a>
</div>
</div>
</div>
</div>
</div>
</div>
</section>





<section class="section">
<div class="container"><!-- Abstract. -->
<div class="columns is-centered has-text-centered">
<div class="column is-two-thirds">
<h2 class="title is-2">Abstract</h2>
<div class="content has-text-justified">

<p>We propose a new class of function approximators based on polynomial expansions. The new class of networks, called &Pi;-nets, express the output as a high-degree polynomial expansion of the input elements.</p>

<p>The unknown parameters, which are naturally represented by high-order tensors, are estimated through a collective tensor factorization with factors sharing. Changing the factor sharing we can obtain diverse architectures tailored to a task at hand. To that end, we derive and implement three tensor decompositions.</p>

<p>We conduct the following diverse experiments: we demonstrate results in image (and pointcloud) generation, image (and audio) classification, face recognition and non-euclidean representation learning. &Pi;-nets are very expressive even in the absence of activation functions. When combined with activation functions between the layers, &Pi;-nets outperform the state-of-the-art in the aforementioned experiments.</p> 
</div>
</div>
</div>
</div>
<!--/ Abstract. --> <!-- Paper video. -->
<div class="columns is-centered has-text-centered">
<div class="column is-two-thirds">
</section>
---
categories:
  - Machine Learning
  - Polynomial Networks
tags:
  - polynomial expansions
  - machine learning
  - research
---

<!-- Images. -->
<div class="columns is-centered">
<div class="column is-full-width">
<h2 class="title is-2">Architectures of polynomial networks</h2>

<p>The idea is to approximate functions using high-degree polynomial expansions. To explain how this would work, let us showcase it with a third-degree polynomial expansion. Then, assuming the input is a d-dimensional vector <strong>z</strong>, we want to capture up to third-degree correlations of the elements of <strong>z</strong>. Let x<sub>t</sub> denote the scalar output of the polynomial expansion and <strong>W</strong><sup>[k]</sup> denote the k<sup>th</sup> degree parameters. Then, the polynomial expansion would be:</p>

<div class="column is-2 has-text-centered"><img class="third-degree-poly" src="/images/pi_nets_third_degree_correlations.png" alt="Third-degree polynomial expansion." />
<br>

<p>The expression above captures all correlations of element z<sub>i</sub> with z<sub>j</sub>, where i, j belong in the [1, d] interval. However, the unknown parameters <strong>W</strong><sup>[k]</sup> scale very fast with respect to the degree of the polynomial. Our goal is to use high-degree expansions on high-dimensional signals, such as images. Therefore, we should reduce the number of unknown parameters.</p>

<p>Tensor decompositions have been effectively used to reduce the number of unknown parameters in the literature. Indeed, we use collective tensor factorization and we can reduce the unknown parameters significantly. In addition, we can obtain simple recursive relationships that enable us to construct arbitrary degree polynomial expansions. The details of the derivations can be found in the papers. </p>

<!-- include latex online: <img src="http://latex.codecogs.com/gif.latex?1+sin(x)" border="0"/> -->

<p>For instance, different architectures that perform polynomial expansions of the input <strong>z</strong>:</p>
<div class="column is-2 has-text-centered"><img class="architecture" src="/images/pi_nets_model_intro.png" alt="Indicative architectures." />
</div>

<p>In the paper, we demonstrate results in a number of tasks, outperforming strong baselines. For instace, you can find some images generated by the proposed &Pi;-nets below:</p>
<div class="column is-2 has-text-centered"><img class="architecture" src="/images/pi_nets_generation_ffhq.png" alt="Indicative architectures." />
</div>

</div>





<section id="BibTeX" class="section">
<div class="container content">
<h2 class="title">BibTeX</h2>
<pre><code>  @article{poly2021,
  author={Chrysos, Grigorios and Moschoglou, Stylianos and Bouritsas, Giorgos and Deng, Jiankang and Panagakis, Yannis and Zafeiriou, Stefanos},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Deep Polynomial Neural Networks}, 
  year={2021},
  pages={1-1},
  doi={10.1109/TPAMI.2021.3058891}}
</code></pre>

<pre><code>  @article{chrysos2019polygan,
  title={Polygan: High-order polynomial generators},
  author={Chrysos, Grigorios and Moschoglou, Stylianos and Panagakis, Yannis and Zafeiriou, Stefanos},
  journal={arXiv preprint arXiv:1908.06571},
  year={2019}
}</code></pre>
</div>
</section>

