---
layout: single
title: Publications
excerpt: "Publications"
share: false
collection: publications
---


<div id="top-publications">

<a href="#journal-papers">Journal papers</a>     <a href="#workshop-papers">Workshop papers</a> 
</div>


<h2 id="peer-reviewed-conferences">Peer-reviewed conference papers:</h2>
<ul>



<!---------------               NeurIPS 2022               --------------->
<li><strong>Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization).</strong>
    <div style="padding-left: 16px">
      <p>Zhenyu Zhu, Fanghui Liu, <strong>Grigorios Chrysos</strong> Volkan Cevher<br>
      Conference on Neural Information Processing Systems (<em>NeurIPS</em>), 2022. <br>
      <a style="color: white; text-decoration: none;" href="https://openreview.net/pdf?id=m8vzptcFKsT"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a> <br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We explore the interplay of the width, the depth and the initialization(s) on the average robustness of neural networks with both theoretical bounds and empirical validation.</span></p> <!-- https://arxiv.org/pdf/2209.07263.pdf -->
<!--  # # Comment it out for now and revisit later.
        <p class="my-hover-target">Bibtex citation.</p>
        <div id="widget" style="display: none;">
          @inproceedings{zhu2022robustness,
                    title={Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization)},
                    author={Zhu, Zhenyu and Liu, Fanghui and Chrysos, G Grigorios and Cevher, Volkan},
                    booktitle={Advances in neural information processing systems (NeurIPS)},
                    year={2022}
          }
        </div>
        <script>
          var p = document.querySelector(".my-hover-target"); // Get a reference to the paragraph element
          var my_widget = document.querySelector("#widget"); // Get a reference to the widget element

          // Add an event listener to the paragraph that
          // shows the widget when the cursor is over the paragraph
          p.addEventListener("mouseover", function() {my_widget.style.display = "block";});

          // Add an event listener to the paragraph that
          // hides the widget when the cursor is no longer over the paragraph
          p.addEventListener("mouseout", function() {my_widget.style.display = "none";});
        </script> -->
    </div></li>




<li><strong>Generalization Properties of NAS under Activation and Skip Connection Search.</strong>
    <div style="padding-left: 16px">
      <p>Zhenyu Zhu, Fanghui Liu, <strong>Grigorios Chrysos</strong> Volkan Cevher<br>
      Conference on Neural Information Processing Systems (<em>NeurIPS</em>), 2022. <br>
      <a style="color: white; text-decoration: none;" href="https://openreview.net/pdf?id=aQySSrCbBul"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a> <br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">Using our theoretical guarantees of neural architecture search (NAS) under various activation functions and residual connections, we design an effective train-free algorithm for NAS.</span></p> <!-- https://arxiv.org/pdf/2209.07238.pdf -->
    </div></li>


<li><strong>Sound and Complete Verification of Polynomial Networks.</strong>
    <div style="padding-left: 16px">
      <p>Elias Abad Rocamora, Mehmet Fatih Sahin, Fanghui Liu, <strong>Grigorios Chrysos</strong> Volkan Cevher<br>
      Conference on Neural Information Processing Systems (<em>NeurIPS</em>), 2022. <br>
      <a style="color: white; text-decoration: none;" href="https://openreview.net/pdf?id=gsdHDI-p6NI"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a>
      <a style="color: white; text-decoration: none;" href="https://github.com/megaelius/PNVerification"><span class="buttong button-round"> <i class="fa fa-fw fa-github" aria-hidden="true"></i>&nbsp;Code</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We propose a branch and bound algorithm for certifying polynomial networks against (adversarial) attacks.</span></p> <!-- https://arxiv.org/pdf/2209.07235.pdf -->
    </div></li>


<li><strong>Extrapolation and Spectral Bias of Neural Nets with Hadamard Product: a Polynomial Net Study.</strong>
    <div style="padding-left: 16px">
      <p>Yongtao Wu, Zhenyu Zhu, Fanghui Liu, <strong>Grigorios Chrysos</strong> Volkan Cevher<br>
      Conference on Neural Information Processing Systems (<em>NeurIPS</em>), 2022. <br>
      <a style="color: white; text-decoration: none;" href="https://openreview.net/pdf?id=_cXUMAnWJJj"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a> <br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We study the extrapolation and spectral bias of neural networks with Hadamard products from a neural tangent kernel perspective.</span></p> <!-- https://arxiv.org/pdf/2209.07736.pdf -->
    </div></li>


<!---------------         End of NeurIPS 2022               --------------->

<li><strong>Augmenting Deep Classifiers with Polynomial Neural Networks.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos*</strong> Markos Georgopoulos*, Jiankang Deng, Jean Kossaifi, Yannis Panagakis, Anima Anandkumar<br>
      European Conference on Computer Vision (<em>ECCV</em>), 2022. <br>
      <a style="color: white; text-decoration: none;" href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850682.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a> <!-- https://arxiv.org/pdf/2104.07916.pdf -->
      <a style="color: white; text-decoration: none;" href="https://github.com/grigorisg9gr/polynomials-for-augmenting-NNs"><span class="buttong button-round"> <i class="fa fa-fw fa-github" aria-hidden="true"></i>&nbsp;Code</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We express modern architectures (e.g., residual and non-local networks) in the form of different degree polynomials of the input. This enables us to design extensions of successful architectures that perform favorably in various benchmarks.</span></p>
    </div></li>


<li><strong>Cluster-guided Image Synthesis with Unconditional Models.</strong>
    <div style="padding-left: 16px">
      <p>Markos Georgopoulos, James Oldfield, <strong>Grigorios Chrysos</strong>, Yannis Panagakis<br>
      Computer Vision and Pattern Recognition Conference (<em>CVPR</em>), 2022. <br>
      <a style="color: white; text-decoration: none;" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Georgopoulos_Cluster-Guided_Image_Synthesis_With_Unconditional_Models_CVPR_2022_paper.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br> <!-- https://arxiv.org/pdf/2112.12911.pdf -->
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We study controllable generation in unsupervised GAN models by leveraging clusters in the representation space of the generator. We show that these clusters, which capture semantic attributes, can be used for conditioning the generator.</span></p>
    </div></li>

<li><strong>The Spectral Bias of Polynomial Neural Networks.</strong>
    <div style="padding-left: 16px">
      <p>Moulik Choraria, Leello Tadesse Dadi, <strong>Grigorios Chrysos</strong>, Julien Mairal, Volkan Cevher<br>
      International Conference on Learning Representations (<em>ICLR</em>), 2022. <br>
      <a style="color: white; text-decoration: none;" href="https://openreview.net/pdf?id=P7FLfMLTSEX"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We study the spectral bias of polynomial networks and compare it with the spectral bias of standard neural nets using kernel approximations.</span></p>
    </div></li>

<li><strong>Controlling the Complexity and Lipschitz Constant improves Polynomial Nets.</strong>
    <div style="padding-left: 16px">
      <p>Zhenyu Zhu, Fabian Latorre, <strong>Grigorios Chrysos</strong>, Volkan Cevher<br>
      International Conference on Learning Representations (<em>ICLR</em>), 2022. <br>
      <a style="color: white; text-decoration: none;" href="https://openreview.net/pdf?id=dQ7Cy_ndl1s"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br> 
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We provide sample complexity results and bounds on the Lipschitz constant of polynomial networks, which we use to construct a regularization scheme that improves the robustness against adversarial noise.</span></p>
    </div></li>

<li><strong>Conditional Generation Using Polynomial Expansions.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Markos Georgopoulos, Yannis Panagakis<br>
      Conference on Neural Information Processing Systems (<em>NeurIPS</em>), 2021. <br>
      <a style="color: white; text-decoration: none;" href="https://proceedings.neurips.cc/paper/2021/file/ef0d3930a7b6c95bd2b32ed45989c61f-Paper.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a> <!-- https://arxiv.org/pdf/2104.05077.pdf -->
      <a style="color: white; text-decoration: none;" href="https://github.com/grigorisg9gr/polynomial_nets_for_conditional_generation"><span class="buttong button-round"> <i class="fa fa-fw fa-github" aria-hidden="true"></i>&nbsp;Code</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We propose a polynomial expansion with respect to two (or more) variables, which is applied to conditional image generation.</span></p>
    </div></li>


<li><strong>Poly-NL: Linear Complexity Non-local Layers with Polynomials.</strong>
    <div style="padding-left: 16px">
      <p>Francesca Babiloni, Ioannis Marras, Filippos Kokkinos, Jiankang Deng, <strong>Grigorios Chrysos</strong>, Stefanos Zafeiriou<br>
      International Conference on Computer Vision (ICCV), 2021. <br>
      <a style="color: white; text-decoration: none;" href="https://arxiv.org/pdf/2107.02859.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We cast non-local blocks as special cases of third degree polynomial functions. In addition, we propose a new non-local block that builds on this polynomial perspective but has more efficient operations, i.e., we aim to retain the expressivity of non-local layers while maintaining a linear complexity.</span></p>
    </div></li>



<li><strong>Tensor Methods in Computer Vision and Deep Learning.</strong>
    <div style="padding-left: 16px">
      <p>Yannis Panagakis*, Jean Kossaifi*, <strong>Grigorios Chrysos</strong>, James Oldfield, Mihalis A. Nicolaou, Anima Anandkumar, Stefanos Zafeiriou<br>
      Proceedings of the IEEE, 2021. <br>
      <a style="color: white; text-decoration: none;" href="https://ieeexplore.ieee.org/document/9420085"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;Paper</span></a> 
      <a style="color: white; text-decoration: none;" href="https://github.com/tensorly/Proceedings_IEEE_companion_notebooks"><span class="buttong button-round"> <i class="fa fa-fw fa-github" aria-hidden="true"></i>&nbsp;Code</span></a> <br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We provide an in-depth review of tensors and tensor methods in the context of representation learning and deep learning, with a particular focus on computer vision applications.  We also provide jupyter notebooks with accompanying code.</span></p>
    </div></li>



<li><strong>Unsupervised Controllable Generation with Self-Training.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Jean Kossaifi, Zhiding Yu, Anima Anandkumar<br>
      International Joint Conference on Neural Networks (IJCNN), 2021.<br>
      <a style="color: white; text-decoration: none;" href="https://arxiv.org/pdf/2007.09250.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a>Oral.<br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We modify the GAN architecture to achieve interpretable generation without using any supervision.</span></p>
    </div></li>




<li><strong>Reconstructing the Noise Manifold for Image Denoising.</strong>
    <div style="padding-left: 16px">
      <p>Ioannis Marras, <strong>Grigorios Chrysos</strong>, Ioannis Alexiou, Gregory Slabaugh, Stefanos Zafeiriou<br>
      European Conference on Computer Vision (<em>ECCV</em>), 2020. <br>
      <a style="color: white; text-decoration: none;" href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123540596.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a> <br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We propose learning the noise variance manifold along with typical image-to-image translation to obtain improved denoising.</span></p>
    </div></li>


<li><strong>Multilinear Latent Conditioning for Generating Unseen Attribute Combinations.</strong>
    <div style="padding-left: 16px">
      <p>Markos Georgopoulos, <strong>Grigorios Chrysos</strong>, Maja Pantic, Yannis Panagakis<br>
      International Conference on Machine Learning (<em>ICML</em>), 2020. <br>
      <a style="color: white; text-decoration: none;" href="http://proceedings.mlr.press/v119/georgopoulos20a/georgopoulos20a.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a> <br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We extend conditional VAE to capture multiplicative interactions of the (annotated) attributes in the latent space. This enables generating images with unseen attribute combinations during training.</span></p>
    </div></li>




<li><strong>&Pi;-nets: Deep Polynomial Neural Networks.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Stylianos Moschoglou, Giorgos Bouritsas, Yannis Panagakis, Jiankang Deng, Stefanos Zafeiriou<br>
      Computer Vision and Pattern Recognition Conference (<em>CVPR</em>), 2020. <br>
      <a style="color: white; text-decoration: none;" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chrysos_P-nets_Deep_Polynomial_Neural_Networks_CVPR_2020_paper.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a> 
      <a style="color: white; text-decoration: none;" href="https://github.com/grigorisg9gr/polynomial_nets"><span class="buttong button-round"> <i class="fa fa-fw fa-github" aria-hidden="true"></i>&nbsp;Code</span></a> 
      <a style="color: white; text-decoration: none;" href="/polynomial-nets/"><span class="buttong button-round"> <i class="fa fa-fw fa-file-text-o" aria-hidden="true"></i>&nbsp;Blog post</span></a>  
      <a style="color: white; text-decoration: none;" href="https://youtu.be/5HmFSoU2cOw"><span class="buttong button-round"> <i class="fa fa-fw fa-file-movie-o" aria-hidden="true"></i>&nbsp;1-minute video</span></a> 
      <a style="color: white; text-decoration: none;" href="/files/publications/pi-net_cvpr_poster.pdf"><span class="buttong button-round"> <i class="fa fa-fw fa-file-text-o" aria-hidden="true"></i>&nbsp;Poster</span></a> <!-- Break --><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We use a high-order polynomial expansion as a function approximation method. The unknown parameters of the polynomial (i.e., high-order tensors) are estimated using a collective tensor factorization.</span></p>
    </div></li>




<li><strong>Robust Conditional Generative Adversarial Networks.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Jean Kossaifi, Stefanos Zafeiriou<br>
      International Conference on Learning Representations (<em>ICLR</em>), 2019.<br>
      <a style="color: white; text-decoration: none;" href="https://arxiv.org/pdf/1805.08657.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a> 
      <a style="color: white; text-decoration: none;" href="https://github.com/grigorisg9gr/rocgan"><span class="buttong button-round"> <i class="fa fa-fw fa-github" aria-hidden="true"></i>&nbsp;Code</span></a>
      <a style="color: white; text-decoration: none;" href="/files/publications/RoCGAN_ICLR_poster.pdf"><span class="buttong button-round"> <i class="fa fa-fw fa-file-text-o" aria-hidden="true"></i>&nbsp;Poster</span></a> <!-- Break --><br><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">The topic of conditional data generation task (e.g., super-resolution) is the focus of this work. We introduce a new pathway in the encoder-decoder generator to improve the synthesized image.</span></p>
    </div></li>




<li><strong>Surface Based Object Detection in RGBD Images.</strong>
    <div style="padding-left: 16px">
      <p>Siddhartha Chandra<em>, <strong>Grigorios Chrysos</strong></em>, Iasonas Kokkinos<br>
      British Machine Vision Conference (<em>BMVC</em>), 2015.<br>
      <a style="color: white; text-decoration: none;" href="https://hal.inria.fr/hal-01263930/document"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a> Oral, acceptance rate: 7%.<br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We extend standard object detection pipelines by leveraging depth information and introducing viewpoint based mixture components.</span></p>
    </div></li>

</ul>




<a href="#top-publications">Return to the top</a>













<!-- *******************************************************************************************
     **********************                 JOURNAL PAPERS                ********************** 
     *******************************************************************************************
-->

<h2 id="journal-papers">Peer-reviewed journal papers:</h2>
<ul>






<li><strong>Linear Complexity Self-Attention with 3rd Order Polynomials.</strong>
    <div style="padding-left: 16px">
      <p>Francesca Babiloni, Ioannis Marras, Filippos Kokkinos, Jiankang Deng, Matteo Maggioni, <strong>Grigorios Chrysos</strong>, Philip Torr, Stefanos Zafeiriou<br>
      IEEE Transactions on Pattern Analysis and Machine Intelligence (<em>T-PAMI</em>), 2023. (impact factor 2019: 17.861). <br>
      <a style="color: white; text-decoration: none;" href="#"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br>  <!-------- TODO: add link -->
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We cast self-attention (and non-local blocks) as special cases of third degree polynomial functions. In addition, we propose a new block that builds on this polynomial perspective but it is more computationally efficient, i.e., we aim to retain the expressivity of self-attention/non-local layers while maintaining a linear complexity.</span></p>
    </div></li>




<li><strong>Revisiting adversarial training for the worst-performing class.</strong>
    <div style="padding-left: 16px">
      <p>Thomas Pethick, <strong>Grigorios Chrysos</strong>, Volkan Cevher<br>
      Transactions on Machine Learning Research (<em>TMLR</em>), 2023. <br>
      <a style="color: white; text-decoration: none;" href="https://openreview.net/forum?id=wkecshlYxI"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br> 
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We propose a new training method called class focused online learning (CFOL) to reduce the gap between the top-performing and worst-performing classes in adversarial training, resulting in a min-max-max optimization formulation.</span></p>
    </div></li>


<!-- * * * * * * * * * *     Pi-nets, PAMI   * * * * * * * * * *  -->
<li><strong>Deep Polynomial Neural Networks.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Stylianos Moschoglou, Giorgos Bouritsas, Jiankang Deng, Yannis Panagakis, Stefanos Zafeiriou<br>
      IEEE Transactions on Pattern Analysis and Machine Intelligence (<em>T-PAMI</em>), 2021. (impact factor 2019: 17.861) <br>
      <a style="color: white; text-decoration: none;" href="https://ieeexplore.ieee.org/document/9353253"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;Paper</span></a> 
      <a style="color: white; text-decoration: none;" href="https://arxiv.org/abs/2006.13026"><span class="buttong button-round">Paper (open access)</span></a> 
      <a style="color: white; text-decoration: none;" href="https://github.com/grigorisg9gr/polynomial_nets"><span class="buttong button-round"> <i class="fa fa-fw fa-github" aria-hidden="true"></i>&nbsp;Code</span></a> 
      <a style="color: white; text-decoration: none;" href="/polynomial-nets/"><span class="buttong button-round"> <i class="fa fa-fw fa-file-text-o" aria-hidden="true"></i>&nbsp;Blog post</span></a>  
      <a style="color: white; text-decoration: none;" href="https://youtu.be/5HmFSoU2cOw"><span class="buttong button-round"> <i class="fa fa-fw fa-file-movie-o" aria-hidden="true"></i>&nbsp;1-minute video</span></a> <!-- Break --><br>

      <span style="color: #609999; font-size: 13px; line-height: 13px;">We propose a new class of architectures that use polynomial expansions to approximate the target functions. We validate the proposed polynomial expansions (i.e. &Pi;-nets) in diverse experiments: data generation, data classifcation, face recognition and non-euclidean representation learning. </span></p>
    </div></li>
<!-- * * * * * * * * * *  end of Pi-nets, PAMI * * * * * * * * * * -->



<li><strong>Non-adversarial polynomial synthesis.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Yannis Panagakis<br>
      Pattern Recognition Letters, 2020. <br>
      <a style="color: white; text-decoration: none;" href="https://www.sciencedirect.com/science/article/abs/pii/S0167865520304116"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;Paper</span></a> <br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We propose a decoder-only generator that uses a polynomial expansion to synthesize new images.</span></p>
    </div></li>



<li><strong>RoCGAN: Robust Conditional GAN.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Jean Kossaifi, Stefanos Zafeiriou<br>
      International Journal of Computer Vision (<em>IJCV</em>), 2020. (impact factor 2019: 11.042) <br>
      <a style="color: white; text-decoration: none;" href="https://link.springer.com/article/10.1007/s11263-020-01348-5"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;Paper (open access)</span></a> 
      <a style="color: white; text-decoration: none;" href="https://github.com/grigorisg9gr/rocgan"><span class="buttong button-round"> <i class="fa fa-fw fa-github" aria-hidden="true"></i>&nbsp;Code</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We leverage structure in the output domain of a conditional data generation task (e.g., super-resolution) to improve the synthesized image. We experimentally validate that this results in synthesized images more robust to noise. Extension of the conference paper.</span></p>
    </div></li>


<li><strong>Motion Deblurring of Faces.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Paolo Favaro, Stefanos Zafeiriou<br>
      International Journal of Computer Vision (<em>IJCV</em>), 2019. (impact factor 2019: 11.042) <br>
      <a style="color: white; text-decoration: none;" href="https://link.springer.com/article/10.1007/s11263-018-1138-7"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;Paper (open access)</span></a> <br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We introduce a framework for tackling motion blur of faces. Our method simulates motion blur using averaging of video frames, while we collect a dataset that contains millions of such frames.</span></p>
    </div></li>

<li><strong>The Menpo Benchmark for Multi-pose 2D and 3D Facial Landmark Localisation and Tracking.</strong>
    <div style="padding-left: 16px">
      <p>Jiankang Deng, Anastasios Roussos, <strong>Grigorios Chrysos</strong>, Evangelos Ververas, Irene Kotsia, Jie Shen, Stefanos Zafeiriou<br>
      International Journal of Computer Vision (<em>IJCV</em>), 2019. (impact factor 2019: 11.042) <br>
      <a style="color: white; text-decoration: none;" href="https://link.springer.com/article/10.1007/s11263-018-1134-y"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;Paper (open access)</span></a> <br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">A semi-automatic framework is proposed for annotating challenging deformable images and videos.</span></p>
    </div></li>


<li><strong>A Comprehensive Performance Evaluation of Deformable Face Tracking ''In-the-Wild''.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Epameinondas Antonakos, Patrick Snape, A. Asthana, Stefanos Zafeiriou<br>
      International Journal of Computer Vision (<em>IJCV</em>), 2018. (impact factor 2019: 11.042)<br>
      <a style="color: white; text-decoration: none;" href="https://link.springer.com/article/10.1007/s11263-017-0999-5"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;Paper (open access)</span></a> 
      <a style="color: white; text-decoration: none;" href="https://github.com/grigorisg9gr/robust_deformable_face_tracking"><span class="buttong button-round"> <i class="fa fa-fw fa-github" aria-hidden="true"></i>&nbsp;Code</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We conduct a large-scale study of deformable face tracking `in-the-wild', i.e., with videos captured in unrestricted conditions.</span></p>
    </div></li>


<li><strong>IPST: Incremental Pictorial Structures for model-free Tracking of deformable objects.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Epameinondas Antonakos, Stefanos Zafeiriou<br>
      IEEE Transactions on Image Processing (<em>TIP</em>), 2018. (impact factor 2019: 9.34)<br>
      <a style="color: white; text-decoration: none;" href="https://ieeexplore.ieee.org/document/8316962"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;Paper</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We introduce incremental pictorial structures for tracking deformable (part-based) objects, e.g., human body parts or fiducial points in the face.</span></p>
    </div></li>


<li><strong>PD2T: Person-specific Detection, Deformable Tracking.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Stefanos Zafeiriou<br>
      IEEE Transactions on Pattern Analysis and Machine Intelligence (<em>T-PAMI</em>), 2017. (impact factor 2019: 17.861)<br>
      <a style="color: white; text-decoration: none;" href="https://ieeexplore.ieee.org/abstract/document/8094942"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;Paper</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We propose a framework for extracting object-specific statistics for tracking a (deformable) object.</span></p>
    </div></li>




</ul>




<a href="#top-publications">Return to the top</a>



<!-- *******************************************************************************************
     **********************                 WORKSHOP PAPERS               ********************** 
     *******************************************************************************************
-->

<h2 id="workshop-papers">Workshop papers:</h2>
<ul>



<li><strong>Self-Supervised Neural Architecture Search for Imbalanced Datasets.</strong>
    <div style="padding-left: 16px">
      <p>Aleksandr Timofeev, <strong>Grigorios Chrysos</strong>, Volkan Cevher<br>
      International Conference on Machine Learning Workshops (ICMLW), 2021.<br>
      <a style="color: white; text-decoration: none;" href="https://arxiv.org/pdf/2109.08580.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We propose a neural architecture search (NAS) framework for real world tasks: (a) in the absence of labels, (b) in the presence of imbalanced datasets, (c) on a constrained computational budget.</span></p>
    </div></li>


<li><strong>Unsupervised Controllable Generation with Self-Training.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Jean Kossaifi, Zhiding Yu, Anima Anandkumar<br>
      International Conference on Machine Learning Workshops (ICMLW), 2020.<br>
      <a style="color: white; text-decoration: none;" href="https://arxiv.org/pdf/2007.09250.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We modify the GAN architecture to achieve interpretable generation without using any supervision.</span></p>
    </div></li>



<li><strong>The 3D Menpo Facial Landmark Tracking Challenge.</strong>
    <div style="padding-left: 16px">
      <p>Stefanos Zafeiriou*<em>, <strong>Grigorios Chrysos</strong>*</em>, Anastasios Roussos*, Evangelos Ververas, J. Deng, George Trigeorgis<br>
      International Conference on Computer Vision Workshops (ICCVW), 2017.<br>
      <a style="color: white; text-decoration: none;" href="https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w36/Zafeiriou_The_3D_Menpo_ICCV_2017_paper.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">The first large-scale dataset with 3D annotations of facial landmarkrs is introduced.</span></p>
    </div></li>


<li><strong>Deep Face Deblurring.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Stefanos Zafeiriou<br>
      Computer Vision and Pattern Recognition Conference Workshops (CVPRW), 2017.<br>
      <a style="color: white; text-decoration: none;" href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Chrysos_Deep_Face_Deblurring_CVPR_2017_paper.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">A method for face deblurring is proposed. The method utilizes weak supervision to guide the learning of the deep neural network.</span></p>
    </div></li>

<li><strong>The Menpo Facial Landmark Localisation Challenge.</strong>
    <div style="padding-left: 16px">
      <p>Stefanos Zafeiriou, George Trigeorgis, <strong>Grigorios Chrysos</strong>, J. Deng, Jie Shen<br>
      Computer Vision and Pattern Recognition Conference Workshops (CVPRW), 2017.<br>
      <a style="color: white; text-decoration: none;" href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w33/papers/Zafeiriou_The_Menpo_Facial_CVPR_2017_paper.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">The first large-scale dataset with annotations of facial landmarkrs in both (semi-)frontal and profile poses is introduced.</span></p>
    </div></li>


<li><strong>The First Facial Landmark Tracking in-the-Wild Challenge: Benchmark and Results.</strong>
    <div style="padding-left: 16px">
      <p>Jie Shen, Stefanos Zafeiriou, <strong>Grigorios Chrysos</strong>, Jean Kossaifi, Georgios Tzimiropoulos, Maja Pantic<br>
      International Conference on Computer Vision Workshops (ICCVW), 2015.<br>
      <a style="color: white; text-decoration: none;" href="https://openaccess.thecvf.com/content_iccv_2015_workshops/w25/papers/Shen_The_First_Facial_ICCV_2015_paper.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">The first large-scale dataset for facial landmark tracking is introduced.</span></p>
    </div></li>


<li><strong>Offline Deformable Face Tracking in Arbitrary Videos.</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong>, Epameinondas Antonakos, Stefanos Zafeiriou, Patrick Snape<br>
      International Conference on Computer Vision Workshops (ICCVW), 2015.<br>
      <a style="color: white; text-decoration: none;" href="https://openaccess.thecvf.com/content_iccv_2015_workshops/w25/papers/Chrysos_Offline_Deformable_Face_ICCV_2015_paper.pdf"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">We propose a framework that can extract object-specific statistics and can be used for tracking long sequences of videos.</span></p>
    </div></li>

</ul>



<h2 id="thesis">Thesis</h2>
<ul>

<li><strong>Polynomial function approximation and its application to deep generative models</strong>
    <div style="padding-left: 16px">
      <p><strong>Grigorios Chrysos</strong><br>
      PhD thesis, Imperial College London. <br>
      <a style="color: white; text-decoration: none;" href="https://doi.org/10.25560/90057"><span class="buttong button-round"> <i class="fa fa-file-pdf-o" aria-hidden="true"></i> &nbsp;PDF</span></a><br>
      <span style="color: #609999; font-size: 13px; line-height: 13px;">My PhD thesis, spanning my first work on polynomial networks and deep generative models.</span></p>
    </div></li>

</ul>


<a href="#top-publications">Return to the top</a>

